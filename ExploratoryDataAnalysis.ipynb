{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project I am going to utilize I94 immmigration dataset and US demographic data to analyze the non immigrants travel patterns. Then I am going to create ETL pipeline using Spark to build a data warehouse using a star table schema for further downstream analysis. I will save results in a Parquet file which could be loaded to S3 or a Redshift cluster if needed. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long\n",
    "\n",
    "from pyspark.sql.functions import udf, date_format, split, col, first, upper\n",
    "\n",
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project I created a data model to analyze the data and discover any relationship patterns between non-immigrant tourists and other metadata available such as demographic information with different preferences in their travel. \n",
    "\n",
    "I am going to use Spark to load and clean the data, parquet files to store outputs, and create a data model using star schema for data warehouse tables.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "### I94 Immigration Data: \n",
    "This data comes from the US National Tourism and Trade Office. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. \n",
    "Some of the important columns are \n",
    "- 1) i94Bir \t Age of non-immigrant in years\n",
    "- 2) i94visa \t Visa codes collapsed into three categories\n",
    "- 3) count \t Used for summary statistics\n",
    "- 4) i94mode \t Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "- 5) i94addr \t USA State of arrival\n",
    "- 6) visatype \t Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "\n",
    "### U.S. City Demographic Data: \n",
    "This data comes from OpenSoft. More details about it can be found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "Some important columns are \n",
    "- 1) City\n",
    "- 2) Median Age (overall median age within the city population)\n",
    "- 3) Total Population\n",
    "- 4) Male Population\n",
    "- 5) Female Population\n",
    "- 6) Count (number of people under specific race category anotated by Race column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df=spark.read.parquet(\"sas_data\")\n",
    "us_immg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "| 438.0|    LOS|20574.0|    1.0|20582.0|  40.0|    1.0|  1.0|     F|94953870030|\n",
      "| 438.0|    LOS|20574.0|    1.0|20591.0|  32.0|    1.0|  1.0|     F|94955622830|\n",
      "| 438.0|    LOS|20574.0|    1.0|20582.0|  29.0|    1.0|  1.0|     M|94956406530|\n",
      "| 438.0|    LOS|20574.0|    1.0|20588.0|  29.0|    1.0|  1.0|     F|94956451430|\n",
      "| 438.0|    LOS|20574.0|    1.0|20588.0|  28.0|    1.0|  1.0|     M|94956388130|\n",
      "| 464.0|    HHW|20574.0|    1.0|20579.0|  57.0|    2.0|  1.0|     M|94981802830|\n",
      "| 464.0|    HHW|20574.0|    1.0|20586.0|  66.0|    2.0|  1.0|     F|94979689930|\n",
      "| 464.0|    HHW|20574.0|    1.0|20586.0|  41.0|    2.0|  1.0|     F|94979746730|\n",
      "| 464.0|    HOU|20574.0|    1.0|20581.0|  27.0|    2.0|  1.0|     M|94973246630|\n",
      "| 464.0|    LOS|20574.0|    1.0|20581.0|  26.0|    2.0|  1.0|     F|95013547930|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df.select(\"i94res\",\"i94port\",\"arrdate\",\"i94mode\",\"depdate\",\"i94bir\",\"i94visa\",\"count\",\"gender\",col(\"admnum\").cast(Long())).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean immigration data\n",
    "- Added a few static list for better readability and dropping duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "|   438|    LOS|  20574|      1|  20582|    40|      1|    1|     F|94953870030|\n",
      "|   438|    LOS|  20574|      1|  20591|    32|      1|    1|     F|94955622830|\n",
      "|   438|    LOS|  20574|      1|  20582|    29|      1|    1|     M|94956406530|\n",
      "|   438|    LOS|  20574|      1|  20588|    29|      1|    1|     F|94956451430|\n",
      "|   438|    LOS|  20574|      1|  20588|    28|      1|    1|     M|94956388130|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df=us_immg_df.select(col(\"i94res\").cast(Int()),col(\"i94port\"),\n",
    "                           col(\"arrdate\").cast(Int()), \\\n",
    "                           col(\"i94mode\").cast(Int()),col(\"depdate\").cast(Int()),\n",
    "                           col(\"i94bir\").cast(Int()),col(\"i94visa\").cast(Int()), \n",
    "                           col(\"count\").cast(Int()), \\\n",
    "                           \"gender\",col(\"admnum\").cast(Long()))\n",
    "us_immg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3096313, 3096302)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_immg_df.count(), us_immg_df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_immg_df.dropDuplicates(['admnum']).count()\n",
    "us_immg_df=us_immg_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "travel_mode =[[1,'Air'],[2,'Sea'],[3,'Land'],[9,'Not Reported']]\n",
    "travel_mode=spark.createDataFrame(travel_mode)\n",
    "travel_mode.write.mode(\"overwrite\").parquet('travel_mode.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa_type = [[1, 'Business'], [2, 'Pleasure'], [3, 'Student']]\n",
    "visa_type=spark.createDataFrame(visa_type)\n",
    "visa_type.write.mode('overwrite').parquet('visa_type.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I used the I94_SAS_Labels_Descriptions.SAS file to extract the port information and residency information. I extracted those to text files which I used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n",
      "| id|           port_city|port_state|\n",
      "+---+--------------------+----------+\n",
      "|ALC|               ALCAN|        AK|\n",
      "|ANC|           ANCHORAGE|        AK|\n",
      "|BAR|BAKER AAF - BAKER...|        AK|\n",
      "|DAC|       DALTONS CACHE|        AK|\n",
      "|PIZ|DEW STATION PT LA...|        AK|\n",
      "+---+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_df = pd.read_csv('ports.txt',sep='=',names=['id','port'])\n",
    "ports_df.head()\n",
    "# Remove whitespaces and single quotes\n",
    "ports_df['id']=ports_df['id'].str.strip().str.replace(\"'\",'')\n",
    "\n",
    "ports_df['port_city'], ports_df['port_state']=ports_df['port'].str.strip().str.replace(\"'\",'').str.strip().str.split(',',1).str\n",
    "\n",
    "ports_df['port_state']=ports_df['port_state'].str.strip()\n",
    "\n",
    "ports_df.drop(columns =['port'], inplace = True)\n",
    "\n",
    "ports_data=ports_df.values.tolist()\n",
    "ports_schema = R([\n",
    "    Fld('id', Str(), True),\n",
    "    Fld('port_city', Str(), True),\n",
    "    Fld('port_state', Str(), True)\n",
    "])\n",
    "ports=spark.createDataFrame(ports_data, ports_schema)\n",
    "ports.show(5)\n",
    "ports.write.mode('overwrite').parquet('./data/ports.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|             country|\n",
      "+---+--------------------+\n",
      "|582|MEXICO Air Sea, a...|\n",
      "|236|         AFGHANISTAN|\n",
      "|101|             ALBANIA|\n",
      "|316|             ALGERIA|\n",
      "|102|             ANDORRA|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities = pd.read_csv('residence_city.txt',sep='=',names=['id','country'])\n",
    "cities['country']=cities['country'].str.replace(\"'\",'').str.strip()\n",
    "cities_data=cities.values.tolist()\n",
    "cities_schema = R([\n",
    "    Fld('id', Str(), True),\n",
    "    Fld('country', Str(), True)\n",
    "])\n",
    "cities=spark.createDataFrame(cities_data, cities_schema)\n",
    "cities.show(5)\n",
    "cities.write.mode('overwrite').parquet('resident_city.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum| id|port_city|port_state|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\n",
      "|   110|    BGM|  20550|      1|  20556|    36|      1|    1|     F|92847893530|BGM|   BANGOR|        ME|\n",
      "|   108|    BGM|  20569|      1|  20571|    43|      1|    1|     M|59234302533|BGM|   BANGOR|        ME|\n",
      "|   129|    BGM|  20559|      1|  20584|    67|      1|    1|     M|93553405030|BGM|   BANGOR|        ME|\n",
      "|   261|    BGM|  20568|      1|   null|     9|      1|    1|     M|94455571030|BGM|   BANGOR|        ME|\n",
      "|   135|    BGM|  20564|      1|  20569|    30|      2|    1|     M|94033156330|BGM|   BANGOR|        ME|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df = us_immg_df.join(ports, us_immg_df.i94port==ports.id, how='left')\n",
    "us_immg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|port_city|port_state|arrival_date|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+\n",
      "|   110|    BGM|  20550|      1|  20556|    36|      1|    1|     F|92847893530|   BANGOR|        ME|  2016-04-06|\n",
      "|   108|    BGM|  20569|      1|  20571|    43|      1|    1|     M|59234302533|   BANGOR|        ME|  2016-04-25|\n",
      "|   129|    BGM|  20559|      1|  20584|    67|      1|    1|     M|93553405030|   BANGOR|        ME|  2016-04-15|\n",
      "|   261|    BGM|  20568|      1|   null|     9|      1|    1|     M|94455571030|   BANGOR|        ME|  2016-04-24|\n",
      "|   135|    BGM|  20564|      1|  20569|    30|      2|    1|     M|94033156330|   BANGOR|        ME|  2016-04-20|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df = us_immg_df.withColumn(\"arrival_date\", get_date(us_immg_df.arrdate))\n",
    "us_immg_df=us_immg_df.drop(\"id\")\n",
    "us_immg_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamp=us_immg_df.select(col('arrdate').alias('arrival_sasdate'),\n",
    "                                   col('arrival_date').alias('arrival_iso_date'),\n",
    "                                   date_format('arrival_date', 'y').alias('arrival_year'), \n",
    "                                   date_format('arrival_date','M').alias('arrival_month'),\n",
    "                                   date_format('arrival_date','E').alias('arrival_dayofweek'), \n",
    "                                   date_format('arrival_date', 'd').alias('arrival_day')).dropDuplicates()\n",
    "us_immg_df.drop('arrival_date').write.mode(\"overwrite\").parquet('us_immigration.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamp.createOrReplaceTempView(\"timestamp\")\n",
    "timestamp=spark.sql('''select arrival_sasdate,\n",
    "                         arrival_iso_date,\n",
    "                         arrival_month,\n",
    "                         arrival_dayofweek,\n",
    "                         arrival_year,\n",
    "                         arrival_day,\n",
    "                         CASE WHEN arrival_month IN (12, 1, 2) THEN 'winter' \n",
    "                                WHEN arrival_month IN (3, 4, 5) THEN 'spring' \n",
    "                                WHEN arrival_month IN (6, 7, 8) THEN 'summer' \n",
    "                                ELSE 'autumn' \n",
    "                         END AS date_season from timestamp''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestamp.write.mode(\"overwrite\").partitionBy(\"arrival_year\", \"arrival_month\").parquet('timestamp.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read and clean Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State',\n",
       " 'Median Age',\n",
       " 'Male Population',\n",
       " 'Female Population',\n",
       " 'Total Population',\n",
       " 'Number of Veterans',\n",
       " 'Foreign-born',\n",
       " 'Average Household Size',\n",
       " 'State Code',\n",
       " 'Race',\n",
       " 'Count']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df=spark.read.csv(\"us-cities-demographics.csv\", sep=';', header=True)\n",
    "demographics_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean Demographics data\n",
    "- Remove duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+---------------+-----------------+----------------+------------+----------------------+\n",
      "|   City|state|Median Age|male population|female population|total population|foreign-born|Average Household Size|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------+----------------------+\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n",
      "|Abilene|Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.select(\"City\",\"state\",\"Median Age\",\"male population\",\"female population\",\"total population\", \\\n",
    "                  \"foreign-born\",\"Average Household Size\").orderBy(\"city\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+\n",
      "|   city|state code|                Race| count|\n",
      "+-------+----------+--------------------+------+\n",
      "|Abilene|        TX|American Indian a...|  1813|\n",
      "|Abilene|        TX|  Hispanic or Latino| 33222|\n",
      "|Abilene|        TX|               White| 95487|\n",
      "|Abilene|        TX|               Asian|  2929|\n",
      "|Abilene|        TX|Black or African-...| 14449|\n",
      "|  Akron|        OH|               White|129192|\n",
      "|  Akron|        OH|  Hispanic or Latino|  3684|\n",
      "|  Akron|        OH|Black or African-...| 66551|\n",
      "|  Akron|        OH|               Asian|  9033|\n",
      "|  Akron|        OH|American Indian a...|  1845|\n",
      "|Alafaya|        FL|  Hispanic or Latino| 34897|\n",
      "|Alafaya|        FL|               Asian| 10336|\n",
      "|Alafaya|        FL|               White| 63666|\n",
      "|Alafaya|        FL|Black or African-...|  6577|\n",
      "|Alameda|        CA|               White| 44232|\n",
      "|Alameda|        CA|American Indian a...|  1329|\n",
      "|Alameda|        CA|Black or African-...|  7364|\n",
      "|Alameda|        CA|  Hispanic or Latino|  8265|\n",
      "|Alameda|        CA|               Asian| 27984|\n",
      "| Albany|        NY|  Hispanic or Latino|  9368|\n",
      "+-------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.select(\"city\",\"state code\",\"Race\",\"count\").orderBy(\"city\").show()\n",
    "race_counts=(demographics_df.select(\"city\",\"state code\",\"Race\",\"count\")\n",
    "    .groupby(demographics_df.City, \"state code\")\n",
    "    .pivot(\"Race\")\n",
    "    .agg(first(\"Count\")))\n",
    "us_dem_data=demographics_df.drop(*[\"Number of Veterans\",\"Race\",\"Count\"]).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|   City|State Code|     State|Median Age|Male Population|Female Population|Total Population|Foreign-born|Average Household Size|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+-------+----------+----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|Abilene|        TX|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|                             1813| 2929|                    14449|             33222| 95487|\n",
      "|  Akron|        OH|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|                             1845| 9033|                    66551|              3684|129192|\n",
      "|Alafaya|        FL|   Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|                             null|10336|                     6577|             34897| 63666|\n",
      "|Alameda|        CA|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|                             1329|27984|                     7364|              8265| 44232|\n",
      "| Albany|        GA|   Georgia|      33.3|          31695|            39414|           71109|         861|                  2.38|                              445|  650|                    53440|              1783| 17160|\n",
      "+-------+----------+----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_dem_data.join(race_counts, [\"city\",\"state code\"]).orderBy(\"city\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------+------+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "|           City|State_Code|MedianAge| Males|Females|TotalPop|Foreign-born|AvgHouseholdSize|NativePopulation|AsianPopulation|BlackPopulation|LatinoPopulation|WhitePoplation|\n",
      "+---------------+----------+---------+------+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "|Highlands Ranch|        CO|     39.6| 49186|  53281|  102467|        8827|            2.72|            1480|           5650|           1779|            8393|         94499|\n",
      "|           Kent|        WA|     33.4| 61825|  65137|  126962|       38175|            3.06|            3651|          26168|          20450|           21928|         67918|\n",
      "|        Madison|        WI|     30.7|122596| 126360|  248956|       30090|            2.23|            2296|          23937|          20424|           19697|        204302|\n",
      "|         Denver|        CO|     34.1|341137| 341408|  682545|      113222|            2.33|           14008|          32491|          72288|          207847|        546370|\n",
      "|         Caguas|        PR|     40.4| 34743|  42265|   77008|        null|            null|             624|           null|           null|           76349|          null|\n",
      "+---------------+----------+---------+------+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_dem_data=us_dem_data.join(race_counts, [\"city\",\"state code\"])\n",
    "us_dem_data=us_dem_data.select('City', col('State Code').alias('State_Code'), 'State', col('Median Age').alias('MedianAge'),\n",
    "     col('Male Population').alias('Males'), col('Female Population').alias('Females'), \n",
    "     col('Total Population').alias('TotalPop'), 'Foreign-born', \n",
    "     col('Average Household Size').alias('AvgHouseholdSize'),\n",
    "     col('American Indian and Alaska Native').alias('NativePopulation'), \n",
    "     col('Asian').alias('AsianPopulation'), \n",
    "     col('Black or African-American').alias('BlackPopulation'), \n",
    "     col('Hispanic or Latino').alias('LatinoPopulation'), \n",
    "     col('White').alias('WhitePoplation'))\n",
    "\n",
    "us_dem_data=us_dem_data.drop(\"state\")\n",
    "us_dem_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "us_dem_data.write.mode('overwrite').parquet(\"us-cities-demographics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_immg_df=us_immg_df.join(us_dem_data, (upper(us_immg_df.port_city)==upper(us_dem_data.City)) & \\\n",
    "                                           (upper(us_immg_df.port_state)==upper(us_dem_data.State_Code)), how='left')\n",
    "us_immg_df.count()\n",
    "us_immg_df=us_immg_df.drop(\"City\",\"State_Code\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+---------+-----+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|port_city|port_state|arrival_date|MedianAge|Males|Females|TotalPop|Foreign-born|AvgHouseholdSize|NativePopulation|AsianPopulation|BlackPopulation|LatinoPopulation|WhitePoplation|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+---------+-----+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "|   103|    NEC|  20556|      3|  20557|    51|      2|    1|     F|  788711085|    NECHE|        ND|  2016-04-12|     null| null|   null|    null|        null|            null|            null|           null|           null|            null|          null|\n",
      "|   112|    NEC|  20573|      3|  20575|    32|      2|    1|     F|59477349333|    NECHE|        ND|  2016-04-29|     null| null|   null|    null|        null|            null|            null|           null|           null|            null|          null|\n",
      "|   245|    LEW|  20574|      3|  20576|    21|      2|    1|     F|94960311530| LEWISTON|        NY|  2016-04-30|     null| null|   null|    null|        null|            null|            null|           null|           null|            null|          null|\n",
      "|   245|    LEW|  20546|      3|   null|    27|      3|    1|     F|71136033030| LEWISTON|        NY|  2016-04-02|     null| null|   null|    null|        null|            null|            null|           null|           null|            null|          null|\n",
      "|   263|    LEW|  20559|      3|  20561|    38|      2|    1|  null|93151263430| LEWISTON|        NY|  2016-04-15|     null| null|   null|    null|        null|            null|            null|           null|           null|            null|          null|\n",
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+------------+---------+-----+-------+--------+------------+----------------+----------------+---------------+---------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_immg_df.count()\n",
    "us_immg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_immg_df.drop('arrival_date').write.mode(\"overwrite\").parquet('i94-immigration-data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "| Table | Type | Description | Columns | \n",
    "| --- | --- | --- | --- | \n",
    "| i94-immigration-data | Fact Table | Stores all the count information about imigration data | i94res, i94port, arrdate, i94mode, depdate, i94bir, i94visa, count, gender, admnum, port_city, port_state, arrival_date, MedianAge, Males, Females, TotalPop, Foreign-born, AvgHouseholdSize, NativePopulation, AsianPopulation, BlackPopulation, LatinoPopulation, WhitePoplation |\n",
    "| resident_city |   Dimension Table | Contains info about resident city of the immigrant | id, country |\n",
    "| ports | Dimension Table  | Port of entry information | id, city, port  |\n",
    "| timestamp |  Dimension Table  | Arrival information |  arrival_sasdate, arrival_iso_date, arrival_month, arrival_dayofweek, arrival_year,arrival_day | \n",
    "| visa |  Dimension Table  | id | id, type | \n",
    "| travel_mode |  Dimension Table  | different travel modes | id, transport | \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "- Load I94 immigation data set and dedupe and drop nulls values. \n",
    "- Select necessary columns for final analysis and clean null values \n",
    "- Build visa type and tranporation mode data frame in spark\n",
    "- Read port of entry data set and residence country statis data from SAS file\n",
    "- Join port of entry data frame and drop the duplicate id columns\n",
    "- Read demographics data set and clean up duplicate and null values\n",
    "- Aggregate results for different races and store them\n",
    "- Join with port of entry data and eventually with immigration data frame. This will be the fact table\n",
    "- Create timestamp dimension table from immigration data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "- Created and etl.py file to do the ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deomographics data successfully persisted\n",
      "Aggregations were successful.\n",
      "Immigration data saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "if us_dem_data.count() > 0:\n",
    "    print('Deomographics data successfully persisted')\n",
    "else:\n",
    "    print('Deomographics data could not be saved!')\n",
    "\n",
    "if us_dem_data.count() == race_counts.count():\n",
    "    print('Aggregations were successful.')\n",
    "else:\n",
    "    print('Failed while aggregating data')\n",
    "\n",
    "if us_immg_df.count() > 0:\n",
    "    print('Immigration data saved successfully')\n",
    "else:\n",
    "    print('No data in immigration data frame')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### timestamp\n",
    "- PK: arrival_sasdate ( arrival date)\n",
    "- Transformed columns such as arrival_month, year, day, season from the above column\n",
    "\n",
    "##### ports\n",
    "- PK: id(port_id)\n",
    "- city: Destination city\n",
    "\n",
    "##### visa\n",
    "- PK: id (visa_id)\n",
    "- type: visa type \n",
    "\n",
    "##### resident_city\n",
    "- PK: id (mode_id)\n",
    "- country\n",
    "\n",
    "##### travel_mode\n",
    "- PK: id (mode_id)\n",
    "- transport: transport type \n",
    "\n",
    "##### i94-immigration-data\n",
    "- PK: admnum (Admission Number)\n",
    "\n",
    "- arrdate (arrival date) FK : timestamp.arrival_sasdate\n",
    "- depdate (Departure Date)\n",
    "- i94port (port id) FK : ports.id\n",
    "- i94res (resident_city) FK : resident_city.id\n",
    "- i94mode (travel mode) FK : travel_mode.id\n",
    "- i94visa (visa id) FK : visa.id\n",
    "- arrdate (arrival date) FK : timestamp.arrival_sasdate\n",
    "- age (age)\n",
    "- gender \n",
    "\n",
    "--- City level aggregations \n",
    "- MedianAge (Median age of the people in the city)\n",
    "- Males (Total male poppulation in the city)\n",
    "- Females (Total female population in the city) \n",
    "- TotalPop\n",
    "- Foreign-born\n",
    "- AvgHouseholdSize \n",
    "\n",
    "--- Race level information about the city. Population of different races based on demographic information\n",
    "- NativePopulation\n",
    "- AsianPopulation \n",
    "- BlackPopulation\n",
    "- LatinoPopulation\n",
    "- WhitePoplation \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "I used Pandas and Seaborn for analysis of smaller files. Whereas for larger files Spark was used. Apart from providing fast and in-memory processing of large datasets Spark also provided support to read SAS files as the immigration data was in this format. Eventually I stored the data in parquet file and sent it to S3 where it could be loaded into Redshift for further analysis.\n",
    "\n",
    "##### Propose how often the data should be updated and why ? \n",
    "There are 2 types of data we dealt with. The demographics and Airport data is more static and won't change much. So we could load this once a month or so. They could be also updated manually if any new data is received. Where are the temperature and immigration data is more dynamic and can be updated daily/weekly or monthly based on downstream requirements. This would also depend on the frequency at which the source datasets are updated. \n",
    "\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " --  We could set up a EMR cluster in AWS and increase the number of nodes to handle the increased data. We could also used cluster manager such as Yarn to manage the resources for us.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " -- Airflow could be used to schedule different steps in the data pipeline. We could setup retries or integrate notification services with pagerduty if any of the steps fails. \n",
    " * The database needed to be accessed by 100+ people.\n",
    " -- The parquet files could be loaded into Amazon Redshift cluster in order to achive high performance. Redshift cluster could be optimized to handle massive parallel requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
